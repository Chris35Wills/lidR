% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/catalog_apply.r
\name{catalog_apply}
\alias{catalog_apply}
\title{Apply a user defined function to an entire catalog in a continuous way}
\usage{
catalog_apply(ctg, func, ...)
}
\arguments{
\item{ctg}{A \link[lidR:catalog]{Catalog} object}

\item{func}{A user's function for which the first input is a LAS object.}

\item{...}{optionnal extra arguments for the user's defined function}
}
\description{
This function enable to apply a user defined routine over an entiere catalog using a multi-core
process. When a user has a dataset organized into several files, it applies a user defined
function to the entiere catalog by automatically splitting the catalog into several clusters.
The clustering pattern can be either split as a set of squared areas or can be split by file.
The clustering pattern can be modified using the global catalog options with \link{catalog_options}.
The "Examples" section describes the procedure to apply functions to the catalog, beginning
with data loading (see example). \cr\cr
\strong{Warning:} there is a buffer mechanism to load buffered data and to avoid edge artifacts
but no mechnism to remove the buffer after applying the user defined functions since this task.
is very specific to each process. See section "Edge artifacts".
}
\section{Edge artifacts}{


It is very important to take precautions to avoid 'edge artifacts' when processing LiDAR
tiles. If the points from neighboring tiles are not included during certain processes,
this could create 'edge artifacts' at the edges of the tiles. For example, empty or incomplete
pixels in a rasterization process. The lidR package provides internal tools to load buffered
data. However there is not mechanism to remove the results computed in the buffered area
since this task depends on the output of the user defined function. Therefore, depending
on what the user is computing some outputs results can appear several time.\cr\cr
The LAS object received by the user defined function has a special colum called 'buffer_side'
which enable, for each point, to know if the point comes from a buffered areas or not. Points
from not buffered areas have a 'buffer_side' of zero while points from buffered areas have
a 'buffer_side' of 1, 2, 3 or 4. 1 being the bottom buffer and 2, 3 and 4 being respectively
the left, top and right buffer (see example).
}

\examples{
# Visit http://jean-romain.github.io/lidR/wiki for an illustrated and commented
# version of this example.
# This example is a dummy exemple. It is more efficient to load the entiere file than
# splitting it in several pieces to process even using sevral cores.

# 1. build a project (here a single file catalog for the need of the example).
folder <- system.file("extdata", "", package="lidR")
project = catalog(folder)
project = project[1,]
plot(project)

# 2. set some global catalog options
# For the need of this dummy example the clustering size is 80 m and the buffer is 15 m using
# a single core (because this example is ran on CRAN server when package is submitted).
catalog_options(buffer = 15, multicore = 1, tiling_size = 80)

# 3. load the shapefile you need to filter your points.
lake_shp = rgdal::readOGR(folder, "lake_polygons_UTM17")

# 4. build the function which analyses each cluster of the catalog.
# This function first argument is a LAS object. The internal routine take care of feeding this
# function. The other arguments can be freely choosen by the user. See the following template:
tree_area = function(las, lake)
{
  # The las argument is a LAS object with each field loaded and an extra column 'buffer'

  # Associate geographic data with lidar points
  lasclassify(las, lake, field = "lake")

  # filter lakes, and low elevation points
  las \%<>\% lasfilter(lake == FALSE, Z > 4)

  # segment trees (in this example the low point density does not enable
  # to segment tree properly. This is just a proof of concept)
  lastrees(las, algorithm = "li2012")

  # Here we used the function tree_metric to compute some metrics for each tree. This
  # function is defined later in the global environnement.
  m = tree_metrics(las, myMetrics(X, Y, Z, buffer))

  # If min buffer is 0 it means the trees were at least parlty in the non buffered area so we
  # want to keep this trees.
  # In an other hand the trees wich are on the edge of the buffered area will be counted
  # twice. So we must remove the tree on the left side and on the bottom side of the buffer
  # If max buffer is <= 2 it means that the trees belong inside the area of interest or in
  # the left side or in the bottom side or both
  m = m[minbuff == 0 & maxbuff <= 2]

  # Remove buffering information which are no longer usefull
  m[, c("minbuff","maxbuff") := NULL]

  return(m)
}

# This function enable to extract, for a single tree, the position of the highest point and
# some informations about the buffing position of the tree. The function tree_metrics take
# care of mapping along each tree.
myMetrics <- function(x, y, z, buff)
{
  i = which.max(z)
  xcenter = x[i]
  ycenter = y[i]
  A = lidR:::area(x,y)  # here we used an internal lidR function not exported for users
  minbuff = min(buff)
  maxbuff = max(buff)

  return(
    list(
      x = xcenter,
      y = ycenter,
      area = A,
      minbuff = minbuff,
      maxbuff = maxbuff
    ))
}

# Evering thing is now well defined, let process over an entiere catalog whith
# hundred of files (but here a single one for the example...)

# 4. Process the project.
output = catalog_apply(project, tree_area, lake = lake_shp)

# 5. Post process the output result (depend on what is the output). Here each values
# of the list is a data.table, so rbindlist makes the job
output = data.table::rbindlist(output)

output \%$\% plot(x,y, cex = sqrt(area/pi)/5, asp = 1)
}
